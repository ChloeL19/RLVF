{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/chloe/mambaforge/envs/python3.9/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/chloe/mambaforge/envs/python3.9/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/chloe/mambaforge/envs/python3.9/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chloe/mambaforge/envs/python3.9/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/chloe/mambaforge/envs/python3.9/lib/libcudart.so.11.0'), PosixPath('/home/chloe/mambaforge/envs/python3.9/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, pipeline\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import json\n",
    "\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfeedback(v):\n",
    "  '''\n",
    "  Returns the compiler error if one exists. Returns None if everything compiles cleanly.\n",
    "  '''\n",
    "  r = requests.post(\"https://coq.livecode.ch/check\", data = { 'v': v }).json()\n",
    "  if r['status'] == 0:\n",
    "    return None\n",
    "  r = r['log']\n",
    "  return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linenumber(cf):\n",
    "  pattern = r'line (\\d+),'\n",
    "  match = re.search(pattern, cf)  \n",
    "  if match:\n",
    "    line_number = int(match.group(1))\n",
    "  else:\n",
    "    line_number = -1\n",
    "  return line_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_totallines(response):\n",
    "    return len(response.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line(line_number, response):\n",
    "    broken = response.split('\\n')\n",
    "    return broken[line_number-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/chloe/.cache/huggingface/datasets/csv/default-d8f13bc6a5969487/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/chloe/.cache/huggingface/datasets/csv/default-d8f13bc6a5969487/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-1598b081ffb826bd.arrow\n"
     ]
    }
   ],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"edbeeching/gpt-neo-125M-imdb-lora-adapter-merged\",\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with='wandb',\n",
    "    mini_batch_size=1,# prev: 16\n",
    "    batch_size=1, # prev: 256, but working with super limited samples so will try lower batch size for now\n",
    "    # gradient_accumulation_steps=1, --> apparently this is unrecognized\n",
    ")\n",
    "\n",
    "# We then define the arguments to pass to the sentiment analysis pipeline.\n",
    "# We set `return_all_scores` to True to get the sentiment score for each token.\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": config.mini_batch_size}\n",
    "\n",
    "def build_dataset(config, dataset_name=\"../MBPP dataset/MBPP_Coq_Test.csv\"):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    ds = load_dataset(\"csv\", data_files=dataset_name, split=\"train\")\n",
    "\n",
    "    def concat(sample):\n",
    "      ex = sample['specification'] + \"Test case 1: \" + sample['test_case_1'] + \\\n",
    "      \", test case 2: \" + sample['test_case_2'] + \", test case 3: \" + sample['test_case_3']\n",
    "      return ex\n",
    "\n",
    "      # return sample['specification'] + \"Test case 1: \" + sample['test_case_1'] + \\\n",
    "      # \", test case 2: \" + sample['test_case_2'] + \", test case 3: \" + sample['test_case_3'] + \" Prove some formal properties. Please only write code for the last stand-alone example. *)\"\n",
    "\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(concat(sample))\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds\n",
    "\n",
    "# multi-shot boilerplate\n",
    "multishot = \"(* Stand-alone Example 1: Write a function that doubles a number. Test case 1: double 3 = 6. Prove some formal properties. *) \\nFixpoint double (n: nat): nat := match n with | 0 => 0 | S n => S (S (double n)) end. \\n\\nLemma example_double_3: double 3 = 6.\\nProof. simpl. reflexivity. Qed. \\n\\n Theorem theorem_double_distribute: \\nforall a b, double a + double b = double (a + b).\\n Proof.\\n intros.\\n induction a.\\n - simpl. reflexivity.\\n - simpl. rewrite IHa. reflexivity. \\n Qed. \\n\\n (* Stand-alone Example 2: Write a function that creates a list of n elements. Test case 1: replicate 1 0 = []. Test case 2: replicate 1 2 = [1; 1]. Prove some formal properties. *) \\n Require Import Coq.Lists.List. \\n Open Scope list_scope. \\n Import ListNotations. \\n Fixpoint replicate {X: Type} (x: X) (n: nat): list X := \\n match n with \\n | 0 => []\\n | S n => x :: replicate x n \\n end. \\n Lemma example_replicate_0: replicate 1 0 = []. \\n Proof. simpl. reflexivity. Qed.\\n Lemma example_replicate_2: replicate 1 2 = [1; 1].\\n Proof. simpl. reflexivity. Qed.\\n\\n Theorem replicate_length:\\n\\t forall n, length (replicate 1 n) = n.\\n Proof. \\n intros. \\n induction n.\\n - simpl. reflexivity. \\n - simpl. rewrite IHn. reflexivity.\\n Qed. \\n Theorem replicate_length_any: \\n\\t forall (X: Type) (x: X) n, length (replicate x n) = n. \\n Proof.\\n intros. \\n induction n.\\n - simpl. reflexivity.\\n- simpl. rewrite IHn. reflexivity.\\n Qed.\"\n",
    "\n",
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "dataset = build_dataset(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load the configuration of '~/data/text-generation-webui/models/llama-7b'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '~/data/text-generation-webui/models/llama-7b' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/configuration_utils.py:628\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    629\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    630\u001b[0m         configuration_file,\n\u001b[1;32m    631\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    632\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    633\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    634\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    635\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    636\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    637\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    638\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    639\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    640\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    641\u001b[0m     )\n\u001b[1;32m    642\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:112\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 112\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    114\u001b[0m \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '~/data/text-generation-webui/models/llama-7b'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM\n\u001b[1;32m      5\u001b[0m base_model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m~/data/text-generation-webui/models/llama-7b\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m LlamaForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      8\u001b[0m         base_model_path, torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16, low_cpu_mem_usage\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[39m=\u001b[39m LlamaTokenizer\u001b[39m.\u001b[39mfrom_pretrained(base_model_path)\n\u001b[1;32m     10\u001b[0m \u001b[39m# model = LlamaForCausalLM.from_pretrained(model_path)\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/modeling_utils.py:2269\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2268\u001b[0m     config_path \u001b[39m=\u001b[39m config \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 2269\u001b[0m     config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mconfig_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m   2270\u001b[0m         config_path,\n\u001b[1;32m   2271\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2272\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2273\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   2274\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   2275\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   2276\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2277\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   2278\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2279\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   2280\u001b[0m         _from_auto\u001b[39m=\u001b[39;49mfrom_auto_class,\n\u001b[1;32m   2281\u001b[0m         _from_pipeline\u001b[39m=\u001b[39;49mfrom_pipeline,\n\u001b[1;32m   2282\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2283\u001b[0m     )\n\u001b[1;32m   2284\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2285\u001b[0m     model_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/configuration_utils.py:546\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPretrainedConfig\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    470\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m     config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    547\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[1;32m    548\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    549\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    550\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m         )\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/configuration_utils.py:573\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    572\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    574\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    575\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/configuration_utils.py:649\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    648\u001b[0m         \u001b[39m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    650\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load the configuration of \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    651\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m from \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    652\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m name. Otherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    653\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m containing a \u001b[39m\u001b[39m{\u001b[39;00mconfiguration_file\u001b[39m}\u001b[39;00m\u001b[39m file\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    654\u001b[0m         )\n\u001b[1;32m    656\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m     \u001b[39m# Load config dict\u001b[39;00m\n\u001b[1;32m    658\u001b[0m     config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_dict_from_json_file(resolved_config_file)\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load the configuration of '~/data/text-generation-webui/models/llama-7b'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '~/data/text-generation-webui/models/llama-7b' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "# loading Llama\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM\n",
    "\n",
    "base_model_path = \"~/data/text-generation-webui/models/llama-7b\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model_path)\n",
    "# model = LlamaForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Llama Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a Coq function to remove first and last occurrence of a given character from the string.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['specification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import LLMChain, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemText = \"\"\" You are an AI assistant helping users write Coq code in order to implement given function specifications. \n",
    "1. The program you write should only contain Coq code in response to the given function specification. \n",
    "3. Any step-by-step reasoning that is not Coq code should be written as a comment.\n",
    "3. As the user provides compiler feedback, modify and update the program accordingly and keep the variables and the general program structure consistent.\n",
    "4. In addition to implementing the function, give at least 2 properties as theorems with their proofs.\n",
    "\n",
    "The following are examples.\n",
    "\n",
    "Query from user:\n",
    "Write a function that creates a list of n elements. Test case 1: replicate 1 0 = []. Test case 2: replicate 1 2 = [1; 1]. \n",
    "\n",
    "Response from assistant:\n",
    "\\`\\`\\`\n",
    "Require Import Coq.Lists.List. \n",
    " Open Scope list_scope. \n",
    " Import ListNotations. \n",
    " Fixpoint replicate (x: X) (n: nat): list X := \n",
    " match n with \n",
    " | 0 => []\n",
    " | S n => x :: replicate x n \n",
    " end. \n",
    " Lemma example_replicate_0: replicate 1 0 = []. \n",
    " Proof. simpl. reflexivity. Qed.\n",
    " Lemma example_replicate_2: replicate 1 2 = [1; 1].\n",
    " Proof. simpl. reflexivity. Qed.\n",
    "\n",
    " Theorem replicate_length:\n",
    "\t forall n, length (replicate 1 n) = n.\n",
    " Proof. \n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity. \n",
    " - simpl. rewrite IHn. reflexivity.\n",
    " Qed. \n",
    " Theorem replicate_length_any: \n",
    "\t forall (X: Type) (x: X) n, length (replicate x n) = n. \n",
    " Proof.\n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity.\n",
    "- simpl. rewrite IHn. reflexivity.\n",
    " Qed.\n",
    "\\`\\`\\`\n",
    "\n",
    "Query from user:\n",
    "Your code produces an error in the line Fixpoint replicate (x: X) (n: nat): list X :=\\n{}Can you please explain what this error means? Let's think step by step. Please rewrite all code if you rewrite any code.\n",
    "File \\\"./ex.v\\\", line 4, characters 24-25:\\nError: The reference X was not found in the current environment.\n",
    "\n",
    "Response from assistant:\n",
    "\\`\\`\\`\n",
    "Require Import Coq.Lists.List. \n",
    " Open Scope list_scope. \n",
    " Import ListNotations. \n",
    " Fixpoint replicate '{X: Type'} (x: X) (n: nat): list X := \n",
    " match n with \n",
    " | 0 => []\n",
    " | S n => x :: replicate x n \n",
    " end. \n",
    " Lemma example_replicate_0: replicate 1 0 = []. \n",
    " Proof. simpl. reflexivity. Qed.\n",
    " Lemma example_replicate_2: replicate 1 2 = [1; 1].\n",
    " Proof. simpl. reflexivity. Qed.\n",
    "\n",
    " Theorem replicate_length:\n",
    "\t forall n, length (replicate 1 n) = n.\n",
    " Proof. \n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity. \n",
    " - simpl. rewrite IHn. reflexivity.\n",
    " Qed. \n",
    " Theorem replicate_length_any: \n",
    "\t forall (X: Type) (x: X) n, length (replicate x n) = n. \n",
    " Proof.\n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity.\n",
    "- simpl. rewrite IHn. reflexivity.\n",
    " Qed.\n",
    "\\`\\`\\`\"\"\"\n",
    "\n",
    "messages=[{\"role\": \"system\", \"content\": systemText}]\n",
    "\n",
    "def generate(q):\n",
    "  '''\n",
    "  Generate output from the correct model and clean it from pre- and post- rambles if possible.\n",
    "  ''' \n",
    "  # make this script retry if the connection is rejected for some reason\n",
    "  while True:\n",
    "    try:\n",
    "      messages.append({\"role\": \"user\", \"content\": q})\n",
    "      response = openai.ChatCompletion.create(\n",
    "                    model='gpt-4-0314', \n",
    "                    messages=messages)\n",
    "      response = response.choices[0].message.content\n",
    "      messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "      \n",
    "      # clean the response if possible\n",
    "      c_response = response\n",
    "      try:\n",
    "        match = re.search('```coq(.*?)```', c_response, re.DOTALL)\n",
    "        c_response = match.group(1)\n",
    "      except:\n",
    "        pass\n",
    "      try:\n",
    "        match = re.search('```(.*?)```', c_response, re.DOTALL)\n",
    "        c_response = match.group(1)\n",
    "      except:\n",
    "        pass\n",
    "      return c_response\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "def run_trial(q_core, pid, outfile, verbose=True, ntrials=10):\n",
    "  '''\n",
    "  Runs one trial on one prompt. \n",
    "  - q: function spec with test cases\n",
    "  - pid: the prompt id\n",
    "  '''\n",
    "  q = q_core\n",
    "  if verbose:\n",
    "    print(\"The task: {}\".format(q))\n",
    "\n",
    "  for t in range(ntrials): \n",
    "    # for recording the dataset\n",
    "    out = {\n",
    "            \"prompt_id\": pid,\n",
    "            \"iteration\": t,\n",
    "            \"instruction\": q,\n",
    "            \"output\": None,\n",
    "            \"compiler_feedback\": None,\n",
    "            \"stats\": {\n",
    "                        \"total_lines\" : None,\n",
    "                        \"compiled_lines\": None,\n",
    "                        \"percent_compiled\": None\n",
    "                    }\n",
    "            }\n",
    "\n",
    "    # generate model response\n",
    "    response = generate(q)\n",
    "\n",
    "    # get compiler feedback\n",
    "    cf = cfeedback(response)\n",
    "\n",
    "    if verbose:\n",
    "      print(\"-----Attempt {}---------\".format(t))\n",
    "      print(response)\n",
    "\n",
    "    if cf is not None:\n",
    "      line_number = get_linenumber(cf) - 1\n",
    "      total_lines = get_totallines(response)\n",
    "      percent_compiled = (line_number)/total_lines\n",
    "      linetxt = get_line(line_number, response)\n",
    "\n",
    "      # get the model to reflect on the error\n",
    "      q = \"Your code produces an error in the line {}\\n{}Can you please explain what this error means? Let's think step by step. Please rewrite all code if you rewrite any code.\"\\\n",
    "        .format(linetxt, cf)\n",
    "      if verbose:\n",
    "        print(q)\n",
    "        print(percent_compiled)\n",
    "    else:\n",
    "      total_lines = get_totallines(response)\n",
    "      line_number = total_lines\n",
    "      percent_compiled = 1.0\n",
    "      q = \"The model solved the problem!\"\n",
    "      if verbose:\n",
    "        print(q)\n",
    "        print(percent_compiled)\n",
    "\n",
    "    # append all data to json lines file\n",
    "    out[\"output\"] = response\n",
    "    out[\"compiler_feedback\"] = cf\n",
    "    out[\"stats\"][\"total_lines\"] = total_lines\n",
    "    out[\"stats\"][\"compiled_lines\"] = line_number\n",
    "    out[\"stats\"][\"percent_compiled\"] = percent_compiled\n",
    "\n",
    "    with open(outfile, 'a') as file:\n",
    "      file.write(json.dumps(out) + '\\n')\n",
    "    if verbose:\n",
    "      print(\"recorded in {}\".format(outfile))\n",
    "\n",
    "    # don't continue if model has completely solved problem\n",
    "    if cf is None:\n",
    "      break\n",
    "\n",
    "  return None\n",
    "\n",
    "def main():\n",
    "  pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n",
    "  outfile = \"Dolly_EasyMediumHard01.ndjson\"\n",
    "  # run_trial(q, 0, outfile)\n",
    "  for i in range(0,3):\n",
    "    q = dataset[i]['query'] \n",
    "    run_trial(q, i, outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
