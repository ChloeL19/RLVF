{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/chloe/mambaforge/envs/python3.9/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/chloe/mambaforge/envs/python3.9/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/chloe/mambaforge/envs/python3.9/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chloe/mambaforge/envs/python3.9/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/chloe/mambaforge/envs/python3.9/lib/libcudart.so.11.0'), PosixPath('/home/chloe/mambaforge/envs/python3.9/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, pipeline\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import json\n",
    "\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfeedback(v):\n",
    "  '''\n",
    "  Returns the compiler error if one exists. Returns None if everything compiles cleanly.\n",
    "  '''\n",
    "  r = requests.post(\"https://coq.livecode.ch/check\", data = { 'v': v }).json()\n",
    "  if r['status'] == 0:\n",
    "    return None\n",
    "  r = r['log']\n",
    "  return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linenumber(cf):\n",
    "  pattern = r'line (\\d+),'\n",
    "  match = re.search(pattern, cf)  \n",
    "  if match:\n",
    "    line_number = int(match.group(1))\n",
    "  else:\n",
    "    line_number = -1\n",
    "  return line_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_totallines(response):\n",
    "    return len(response.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line(line_number, response):\n",
    "    broken = response.split('\\n')\n",
    "    return broken[line_number-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/chloe/.cache/huggingface/datasets/csv/default-d8f13bc6a5969487/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/chloe/.cache/huggingface/datasets/csv/default-d8f13bc6a5969487/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-1598b081ffb826bd.arrow\n"
     ]
    }
   ],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"edbeeching/gpt-neo-125M-imdb-lora-adapter-merged\",\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with='wandb',\n",
    "    mini_batch_size=1,# prev: 16\n",
    "    batch_size=1, # prev: 256, but working with super limited samples so will try lower batch size for now\n",
    "    # gradient_accumulation_steps=1, --> apparently this is unrecognized\n",
    ")\n",
    "\n",
    "# We then define the arguments to pass to the sentiment analysis pipeline.\n",
    "# We set `return_all_scores` to True to get the sentiment score for each token.\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": config.mini_batch_size}\n",
    "\n",
    "def build_dataset(config, dataset_name=\"../MBPP dataset/MBPP_Coq_Test.csv\"):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    ds = load_dataset(\"csv\", data_files=dataset_name, split=\"train\")\n",
    "\n",
    "    def concat(sample):\n",
    "      ex = sample['specification'] + \"Test case 1: \" + sample['test_case_1'] + \\\n",
    "      \", test case 2: \" + sample['test_case_2'] + \", test case 3: \" + sample['test_case_3']\n",
    "      return ex\n",
    "\n",
    "      # return sample['specification'] + \"Test case 1: \" + sample['test_case_1'] + \\\n",
    "      # \", test case 2: \" + sample['test_case_2'] + \", test case 3: \" + sample['test_case_3'] + \" Prove some formal properties. Please only write code for the last stand-alone example. *)\"\n",
    "\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(concat(sample))\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds\n",
    "\n",
    "# multi-shot boilerplate\n",
    "multishot = \"(* Stand-alone Example 1: Write a function that doubles a number. Test case 1: double 3 = 6. Prove some formal properties. *) \\nFixpoint double (n: nat): nat := match n with | 0 => 0 | S n => S (S (double n)) end. \\n\\nLemma example_double_3: double 3 = 6.\\nProof. simpl. reflexivity. Qed. \\n\\n Theorem theorem_double_distribute: \\nforall a b, double a + double b = double (a + b).\\n Proof.\\n intros.\\n induction a.\\n - simpl. reflexivity.\\n - simpl. rewrite IHa. reflexivity. \\n Qed. \\n\\n (* Stand-alone Example 2: Write a function that creates a list of n elements. Test case 1: replicate 1 0 = []. Test case 2: replicate 1 2 = [1; 1]. Prove some formal properties. *) \\n Require Import Coq.Lists.List. \\n Open Scope list_scope. \\n Import ListNotations. \\n Fixpoint replicate {X: Type} (x: X) (n: nat): list X := \\n match n with \\n | 0 => []\\n | S n => x :: replicate x n \\n end. \\n Lemma example_replicate_0: replicate 1 0 = []. \\n Proof. simpl. reflexivity. Qed.\\n Lemma example_replicate_2: replicate 1 2 = [1; 1].\\n Proof. simpl. reflexivity. Qed.\\n\\n Theorem replicate_length:\\n\\t forall n, length (replicate 1 n) = n.\\n Proof. \\n intros. \\n induction n.\\n - simpl. reflexivity. \\n - simpl. rewrite IHn. reflexivity.\\n Qed. \\n Theorem replicate_length_any: \\n\\t forall (X: Type) (x: X) n, length (replicate x n) = n. \\n Proof.\\n intros. \\n induction n.\\n - simpl. reflexivity.\\n- simpl. rewrite IHn. reflexivity.\\n Qed.\"\n",
    "\n",
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "dataset = build_dataset(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def report_gpu():\n",
    "   print(torch.cuda.list_gpu_processes())\n",
    "   gc.collect()\n",
    "   torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemText = \"\"\" You are an AI assistant helping users write Coq code in order to implement given function specifications. \n",
    "1. The program you write should only contain Coq code in response to the given function specification. \n",
    "3. Any step-by-step reasoning that is not Coq code should be written as a comment.\n",
    "3. As the user provides compiler feedback, modify and update the program accordingly and keep the variables and the general program structure consistent.\n",
    "4. In addition to implementing the function, give at least 2 properties as theorems with their proofs.\n",
    "\n",
    "The following are examples.\n",
    "\n",
    "### Human:\n",
    "Write a function that creates a list of n elements. Test case 1: replicate 1 0 = []. Test case 2: replicate 1 2 = [1; 1]. \n",
    "\n",
    "### Assistant:\n",
    "\\`\\`\\`\n",
    "Require Import Coq.Lists.List. \n",
    " Open Scope list_scope. \n",
    " Import ListNotations. \n",
    " Fixpoint replicate (x: X) (n: nat): list X := \n",
    " match n with \n",
    " | 0 => []\n",
    " | S n => x :: replicate x n \n",
    " end. \n",
    " Lemma example_replicate_0: replicate 1 0 = []. \n",
    " Proof. simpl. reflexivity. Qed.\n",
    " Lemma example_replicate_2: replicate 1 2 = [1; 1].\n",
    " Proof. simpl. reflexivity. Qed.\n",
    "\n",
    " Theorem replicate_length:\n",
    "\t forall n, length (replicate 1 n) = n.\n",
    " Proof. \n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity. \n",
    " - simpl. rewrite IHn. reflexivity.\n",
    " Qed. \n",
    " Theorem replicate_length_any: \n",
    "\t forall (X: Type) (x: X) n, length (replicate x n) = n. \n",
    " Proof.\n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity.\n",
    "- simpl. rewrite IHn. reflexivity.\n",
    " Qed.\n",
    "\\`\\`\\`\n",
    "\n",
    "### Human:\n",
    "Your code produces an error in the line Fixpoint replicate (x: X) (n: nat): list X :=\\n{}Can you please explain what this error means? Let's think step by step. Please rewrite all code if you rewrite any code.\n",
    "File \\\"./ex.v\\\", line 4, characters 24-25:\\nError: The reference X was not found in the current environment.\n",
    "\n",
    "### Assistant:\n",
    "\\`\\`\\`\n",
    "Require Import Coq.Lists.List. \n",
    " Open Scope list_scope. \n",
    " Import ListNotations. \n",
    " Fixpoint replicate '{X: Type'} (x: X) (n: nat): list X := \n",
    " match n with \n",
    " | 0 => []\n",
    " | S n => x :: replicate x n \n",
    " end. \n",
    " Lemma example_replicate_0: replicate 1 0 = []. \n",
    " Proof. simpl. reflexivity. Qed.\n",
    " Lemma example_replicate_2: replicate 1 2 = [1; 1].\n",
    " Proof. simpl. reflexivity. Qed.\n",
    "\n",
    " Theorem replicate_length:\n",
    "\t forall n, length (replicate 1 n) = n.\n",
    " Proof. \n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity. \n",
    " - simpl. rewrite IHn. reflexivity.\n",
    " Qed. \n",
    " Theorem replicate_length_any: \n",
    "\t forall (X: Type) (x: X) n, length (replicate x n) = n. \n",
    " Proof.\n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity.\n",
    "- simpl. rewrite IHn. reflexivity.\n",
    " Qed.\n",
    "\\`\\`\\`\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f89ff65c90e41da97050307038b9b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GenerationConfig, LlamaTokenizer, BitsAndBytesConfig\n",
    "\n",
    "base_model = \"/data/text-generation-webui/models/vicuna-7b\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    "    \n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 500.00 MiB (GPU 0; 23.70 GiB total capacity; 22.01 GiB already allocated; 171.19 MiB free; 22.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     18\u001b[0m generate_params \u001b[39m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: input_ids,\n\u001b[1;32m     20\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration_config\u001b[39m\u001b[39m\"\u001b[39m: generation_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmax_new_tokens\u001b[39m\u001b[39m\"\u001b[39m: max_new_tokens,\n\u001b[1;32m     24\u001b[0m         }\n\u001b[1;32m     25\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 26\u001b[0m         generation_output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     27\u001b[0m                 input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     28\u001b[0m                 generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m     29\u001b[0m                 return_dict_in_generate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     30\u001b[0m                 output_scores\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     31\u001b[0m                 max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens,\n\u001b[1;32m     32\u001b[0m             )\n\u001b[1;32m     33\u001b[0m         s \u001b[39m=\u001b[39m generation_output\u001b[39m.\u001b[39msequences[\u001b[39m0\u001b[39m]\n\u001b[1;32m     34\u001b[0m         output \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(s)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/generation/utils.py:1524\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1523\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1524\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1525\u001b[0m         input_ids,\n\u001b[1;32m   1526\u001b[0m         beam_scorer,\n\u001b[1;32m   1527\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1528\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1529\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1530\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1531\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1532\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1533\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1534\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1535\u001b[0m     )\n\u001b[1;32m   1537\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1538\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/generation/utils.py:2810\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2806\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   2808\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2810\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2811\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2812\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2813\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2814\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2815\u001b[0m )\n\u001b[1;32m   2817\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2818\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:700\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    687\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\n\u001b[1;32m    688\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    689\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    696\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    697\u001b[0m )\n\u001b[1;32m    699\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 700\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlm_head(hidden_states)\n\u001b[1;32m    702\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39m# Shift so that tokens < n predict n\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/accelerate/hooks.py:160\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(old_forward)\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_forward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 160\u001b[0m     args, kwargs \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_hf_hook\u001b[39m.\u001b[39;49mpre_forward(module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    161\u001b[0m     \u001b[39mif\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mno_grad:\n\u001b[1;32m    162\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/accelerate/hooks.py:280\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffload:\n\u001b[1;32m    277\u001b[0m     \u001b[39mfor\u001b[39;00m name, _ \u001b[39min\u001b[39;00m named_module_tensors(\n\u001b[1;32m    278\u001b[0m         module, include_buffers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffload_buffers, recurse\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplace_submodules\n\u001b[1;32m    279\u001b[0m     ):\n\u001b[0;32m--> 280\u001b[0m         set_module_tensor_to_device(module, name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecution_device, value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights_map[name])\n\u001b[1;32m    282\u001b[0m \u001b[39mreturn\u001b[39;00m send_to_device(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecution_device), send_to_device(kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecution_device)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/accelerate/utils/modeling.py:149\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype)\u001b[0m\n\u001b[1;32m    147\u001b[0m     new_value \u001b[39m=\u001b[39m old_value\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    148\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 149\u001b[0m     new_value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     new_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(value, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 500.00 MiB (GPU 0; 23.70 GiB total capacity; 22.01 GiB already allocated; 171.19 MiB free; 22.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "prompt = \"### Human: Write a Coq function to reverse a list.\\n###Assistant: \"\n",
    "device = \"cuda\"\n",
    "temperature = 0.7\n",
    "top_p = 0.75\n",
    "top_k = 40\n",
    "num_beams = 4\n",
    "max_new_tokens = 128\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams\n",
    "        )\n",
    "\n",
    "generate_params = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"generation_config\": generation_config,\n",
    "            \"return_dict_in_generate\": True,\n",
    "            \"output_scores\": True,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "        }\n",
    "with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                generation_config=generation_config,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "        s = generation_output.sequences[0]\n",
    "        output = tokenizer.decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Human: what color is the sky?\n",
      "###Assistant: \n",
      "\n",
      "The color of the sky can vary depending on the time of day and the weather conditions. During the daytime, when the sun is shining, the sky is usually a bright blue color. However, when the sun is setting or when there are clouds in the sky, the color of the sky can range from pink to orange to purple.</s><s>\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Llama Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a Coq function to remove first and last occurrence of a given character from the string.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstOne = dataset[0]['specification']\n",
    "firstOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemText = \"\"\" You are an AI assistant helping users write Coq code in order to implement given function specifications. \n",
    "1. The program you write should only contain Coq code in response to the given function specification. \n",
    "3. Any step-by-step reasoning that is not Coq code should be written as a comment.\n",
    "3. As the user provides compiler feedback, modify and update the program accordingly and keep the variables and the general program structure consistent.\n",
    "4. In addition to implementing the function, give at least 2 properties as theorems with their proofs.\n",
    "\n",
    "The following are examples.\n",
    "\n",
    "Query from user:\n",
    "Write a function that creates a list of n elements. Test case 1: replicate 1 0 = []. Test case 2: replicate 1 2 = [1; 1]. \n",
    "\n",
    "Response from assistant:\n",
    "\\`\\`\\`\n",
    "Require Import Coq.Lists.List. \n",
    " Open Scope list_scope. \n",
    " Import ListNotations. \n",
    " Fixpoint replicate (x: X) (n: nat): list X := \n",
    " match n with \n",
    " | 0 => []\n",
    " | S n => x :: replicate x n \n",
    " end. \n",
    " Lemma example_replicate_0: replicate 1 0 = []. \n",
    " Proof. simpl. reflexivity. Qed.\n",
    " Lemma example_replicate_2: replicate 1 2 = [1; 1].\n",
    " Proof. simpl. reflexivity. Qed.\n",
    "\n",
    " Theorem replicate_length:\n",
    "\t forall n, length (replicate 1 n) = n.\n",
    " Proof. \n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity. \n",
    " - simpl. rewrite IHn. reflexivity.\n",
    " Qed. \n",
    " Theorem replicate_length_any: \n",
    "\t forall (X: Type) (x: X) n, length (replicate x n) = n. \n",
    " Proof.\n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity.\n",
    "- simpl. rewrite IHn. reflexivity.\n",
    " Qed.\n",
    "\\`\\`\\`\n",
    "\n",
    "Query from user:\n",
    "Your code produces an error in the line Fixpoint replicate (x: X) (n: nat): list X :=\\n{}Can you please explain what this error means? Let's think step by step. Please rewrite all code if you rewrite any code.\n",
    "File \\\"./ex.v\\\", line 4, characters 24-25:\\nError: The reference X was not found in the current environment.\n",
    "\n",
    "Response from assistant:\n",
    "\\`\\`\\`\n",
    "Require Import Coq.Lists.List. \n",
    " Open Scope list_scope. \n",
    " Import ListNotations. \n",
    " Fixpoint replicate '{X: Type'} (x: X) (n: nat): list X := \n",
    " match n with \n",
    " | 0 => []\n",
    " | S n => x :: replicate x n \n",
    " end. \n",
    " Lemma example_replicate_0: replicate 1 0 = []. \n",
    " Proof. simpl. reflexivity. Qed.\n",
    " Lemma example_replicate_2: replicate 1 2 = [1; 1].\n",
    " Proof. simpl. reflexivity. Qed.\n",
    "\n",
    " Theorem replicate_length:\n",
    "\t forall n, length (replicate 1 n) = n.\n",
    " Proof. \n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity. \n",
    " - simpl. rewrite IHn. reflexivity.\n",
    " Qed. \n",
    " Theorem replicate_length_any: \n",
    "\t forall (X: Type) (x: X) n, length (replicate x n) = n. \n",
    " Proof.\n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity.\n",
    "- simpl. rewrite IHn. reflexivity.\n",
    " Qed.\n",
    "\\`\\`\\`\"\"\"\n",
    "\n",
    "messages=[{\"role\": \"system\", \"content\": systemText}]\n",
    "\n",
    "def generate(q):\n",
    "  '''\n",
    "  Generate output from the correct model and clean it from pre- and post- rambles if possible.\n",
    "  ''' \n",
    "  # make this script retry if the connection is rejected for some reason\n",
    "  while True:\n",
    "    try:\n",
    "      messages.append({\"role\": \"user\", \"content\": q})\n",
    "      response = openai.ChatCompletion.create(\n",
    "                    model='gpt-4-0314', \n",
    "                    messages=messages)\n",
    "      response = response.choices[0].message.content\n",
    "      messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "      \n",
    "      # clean the response if possible\n",
    "      c_response = response\n",
    "      try:\n",
    "        match = re.search('```coq(.*?)```', c_response, re.DOTALL)\n",
    "        c_response = match.group(1)\n",
    "      except:\n",
    "        pass\n",
    "      try:\n",
    "        match = re.search('```(.*?)```', c_response, re.DOTALL)\n",
    "        c_response = match.group(1)\n",
    "      except:\n",
    "        pass\n",
    "      return c_response\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "def run_trial(q_core, pid, outfile, verbose=True, ntrials=10):\n",
    "  '''\n",
    "  Runs one trial on one prompt. \n",
    "  - q: function spec with test cases\n",
    "  - pid: the prompt id\n",
    "  '''\n",
    "  q = q_core\n",
    "  if verbose:\n",
    "    print(\"The task: {}\".format(q))\n",
    "\n",
    "  for t in range(ntrials): \n",
    "    # for recording the dataset\n",
    "    out = {\n",
    "            \"prompt_id\": pid,\n",
    "            \"iteration\": t,\n",
    "            \"instruction\": q,\n",
    "            \"output\": None,\n",
    "            \"compiler_feedback\": None,\n",
    "            \"stats\": {\n",
    "                        \"total_lines\" : None,\n",
    "                        \"compiled_lines\": None,\n",
    "                        \"percent_compiled\": None\n",
    "                    }\n",
    "            }\n",
    "\n",
    "    # generate model response\n",
    "    response = generate(q)\n",
    "\n",
    "    # get compiler feedback\n",
    "    cf = cfeedback(response)\n",
    "\n",
    "    if verbose:\n",
    "      print(\"-----Attempt {}---------\".format(t))\n",
    "      print(response)\n",
    "\n",
    "    if cf is not None:\n",
    "      line_number = get_linenumber(cf) - 1\n",
    "      total_lines = get_totallines(response)\n",
    "      percent_compiled = (line_number)/total_lines\n",
    "      linetxt = get_line(line_number, response)\n",
    "\n",
    "      # get the model to reflect on the error\n",
    "      q = \"Your code produces an error in the line {}\\n{}Can you please explain what this error means? Let's think step by step. Please rewrite all code if you rewrite any code.\"\\\n",
    "        .format(linetxt, cf)\n",
    "      if verbose:\n",
    "        print(q)\n",
    "        print(percent_compiled)\n",
    "    else:\n",
    "      total_lines = get_totallines(response)\n",
    "      line_number = total_lines\n",
    "      percent_compiled = 1.0\n",
    "      q = \"The model solved the problem!\"\n",
    "      if verbose:\n",
    "        print(q)\n",
    "        print(percent_compiled)\n",
    "\n",
    "    # append all data to json lines file\n",
    "    out[\"output\"] = response\n",
    "    out[\"compiler_feedback\"] = cf\n",
    "    out[\"stats\"][\"total_lines\"] = total_lines\n",
    "    out[\"stats\"][\"compiled_lines\"] = line_number\n",
    "    out[\"stats\"][\"percent_compiled\"] = percent_compiled\n",
    "\n",
    "    with open(outfile, 'a') as file:\n",
    "      file.write(json.dumps(out) + '\\n')\n",
    "    if verbose:\n",
    "      print(\"recorded in {}\".format(outfile))\n",
    "\n",
    "    # don't continue if model has completely solved problem\n",
    "    if cf is None:\n",
    "      break\n",
    "\n",
    "  return None\n",
    "\n",
    "def main():\n",
    "  pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n",
    "  outfile = \"Dolly_EasyMediumHard01.ndjson\"\n",
    "  # run_trial(q, 0, outfile)\n",
    "  for i in range(0,3):\n",
    "    q = dataset[i]['query'] \n",
    "    run_trial(q, i, outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
