{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/chloe/mambaforge/envs/python3.9/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/chloe/mambaforge/envs/python3.9/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/chloe/mambaforge/envs/python3.9/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chloe/mambaforge/envs/python3.9/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/chloe/mambaforge/envs/python3.9/lib/libcudart.so'), PosixPath('/home/chloe/mambaforge/envs/python3.9/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, pipeline\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import json\n",
    "\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfeedback(v):\n",
    "  '''\n",
    "  Returns the compiler error if one exists. Returns None if everything compiles cleanly.\n",
    "  '''\n",
    "  r = requests.post(\"https://coq.livecode.ch/check\", data = { 'v': v }).json()\n",
    "  if r['status'] == 0:\n",
    "    return None\n",
    "  r = r['log']\n",
    "  return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linenumber(cf):\n",
    "  pattern = r'line (\\d+),'\n",
    "  match = re.search(pattern, cf)  \n",
    "  if match:\n",
    "    line_number = int(match.group(1))\n",
    "  else:\n",
    "    line_number = -1\n",
    "  return line_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_totallines(response):\n",
    "    return len(response.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line(line_number, response):\n",
    "    broken = response.split('\\n')\n",
    "    return broken[line_number-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/chloe/.cache/huggingface/datasets/csv/default-d8f13bc6a5969487/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/chloe/.cache/huggingface/datasets/csv/default-d8f13bc6a5969487/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-1598b081ffb826bd.arrow\n"
     ]
    }
   ],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"edbeeching/gpt-neo-125M-imdb-lora-adapter-merged\",\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with='wandb',\n",
    "    mini_batch_size=1,# prev: 16\n",
    "    batch_size=1, # prev: 256, but working with super limited samples so will try lower batch size for now\n",
    "    # gradient_accumulation_steps=1, --> apparently this is unrecognized\n",
    ")\n",
    "\n",
    "# We then define the arguments to pass to the sentiment analysis pipeline.\n",
    "# We set `return_all_scores` to True to get the sentiment score for each token.\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": config.mini_batch_size}\n",
    "\n",
    "def build_dataset(config, dataset_name=\"../MBPP dataset/MBPP_Coq_Test.csv\"):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    ds = load_dataset(\"csv\", data_files=dataset_name, split=\"train\")\n",
    "\n",
    "    def concat(sample):\n",
    "      ex = sample['specification'] + \"Test case 1: \" + sample['test_case_1'] + \\\n",
    "      \", test case 2: \" + sample['test_case_2'] + \", test case 3: \" + sample['test_case_3']\n",
    "      return ex\n",
    "\n",
    "      # return sample['specification'] + \"Test case 1: \" + sample['test_case_1'] + \\\n",
    "      # \", test case 2: \" + sample['test_case_2'] + \", test case 3: \" + sample['test_case_3'] + \" Prove some formal properties. Please only write code for the last stand-alone example. *)\"\n",
    "\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(concat(sample))\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds\n",
    "\n",
    "# multi-shot boilerplate\n",
    "multishot = \"(* Stand-alone Example 1: Write a function that doubles a number. Test case 1: double 3 = 6. Prove some formal properties. *) \\nFixpoint double (n: nat): nat := match n with | 0 => 0 | S n => S (S (double n)) end. \\n\\nLemma example_double_3: double 3 = 6.\\nProof. simpl. reflexivity. Qed. \\n\\n Theorem theorem_double_distribute: \\nforall a b, double a + double b = double (a + b).\\n Proof.\\n intros.\\n induction a.\\n - simpl. reflexivity.\\n - simpl. rewrite IHa. reflexivity. \\n Qed. \\n\\n (* Stand-alone Example 2: Write a function that creates a list of n elements. Test case 1: replicate 1 0 = []. Test case 2: replicate 1 2 = [1; 1]. Prove some formal properties. *) \\n Require Import Coq.Lists.List. \\n Open Scope list_scope. \\n Import ListNotations. \\n Fixpoint replicate {X: Type} (x: X) (n: nat): list X := \\n match n with \\n | 0 => []\\n | S n => x :: replicate x n \\n end. \\n Lemma example_replicate_0: replicate 1 0 = []. \\n Proof. simpl. reflexivity. Qed.\\n Lemma example_replicate_2: replicate 1 2 = [1; 1].\\n Proof. simpl. reflexivity. Qed.\\n\\n Theorem replicate_length:\\n\\t forall n, length (replicate 1 n) = n.\\n Proof. \\n intros. \\n induction n.\\n - simpl. reflexivity. \\n - simpl. rewrite IHn. reflexivity.\\n Qed. \\n Theorem replicate_length_any: \\n\\t forall (X: Type) (x: X) n, length (replicate x n) = n. \\n Proof.\\n intros. \\n induction n.\\n - simpl. reflexivity.\\n- simpl. rewrite IHn. reflexivity.\\n Qed.\"\n",
    "\n",
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "dataset = build_dataset(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Manually trigger Python's garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# Clear the CUDA cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemText = \"\"\" You are an AI assistant helping users write Coq code in order to implement given function specifications. \n",
    "1. The program you write should only contain Coq code in response to the given function specification. \n",
    "3. Any step-by-step reasoning that is not Coq code should be written as a comment.\n",
    "3. As the user provides compiler feedback, modify and update the program accordingly and keep the variables and the general program structure consistent.\n",
    "4. In addition to implementing the function, give at least 2 properties as theorems with their proofs.\n",
    "\n",
    "The following are examples.\n",
    "\n",
    "### Human:\n",
    "Write a function that creates a list of n elements. Test case 1: replicate 1 0 = []. Test case 2: replicate 1 2 = [1; 1]. \n",
    "\n",
    "### Assistant:\n",
    "\\`\\`\\`\n",
    "Require Import Coq.Lists.List. \n",
    " Open Scope list_scope. \n",
    " Import ListNotations. \n",
    " Fixpoint replicate (x: X) (n: nat): list X := \n",
    " match n with \n",
    " | 0 => []\n",
    " | S n => x :: replicate x n \n",
    " end. \n",
    " Lemma example_replicate_0: replicate 1 0 = []. \n",
    " Proof. simpl. reflexivity. Qed.\n",
    " Lemma example_replicate_2: replicate 1 2 = [1; 1].\n",
    " Proof. simpl. reflexivity. Qed.\n",
    "\n",
    " Theorem replicate_length:\n",
    "\t forall n, length (replicate 1 n) = n.\n",
    " Proof. \n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity. \n",
    " - simpl. rewrite IHn. reflexivity.\n",
    " Qed. \n",
    " Theorem replicate_length_any: \n",
    "\t forall (X: Type) (x: X) n, length (replicate x n) = n. \n",
    " Proof.\n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity.\n",
    "- simpl. rewrite IHn. reflexivity.\n",
    " Qed.\n",
    "\\`\\`\\`\n",
    "\n",
    "### Human:\n",
    "Your code produces an error in the line Fixpoint replicate (x: X) (n: nat): list X :=\\n{}Can you please explain what this error means? Let's think step by step. Please rewrite all code if you rewrite any code.\n",
    "File \\\"./ex.v\\\", line 4, characters 24-25:\\nError: The reference X was not found in the current environment.\n",
    "\n",
    "### Assistant:\n",
    "\\`\\`\\`\n",
    "Require Import Coq.Lists.List. \n",
    " Open Scope list_scope. \n",
    " Import ListNotations. \n",
    " Fixpoint replicate '{X: Type'} (x: X) (n: nat): list X := \n",
    " match n with \n",
    " | 0 => []\n",
    " | S n => x :: replicate x n \n",
    " end. \n",
    " Lemma example_replicate_0: replicate 1 0 = []. \n",
    " Proof. simpl. reflexivity. Qed.\n",
    " Lemma example_replicate_2: replicate 1 2 = [1; 1].\n",
    " Proof. simpl. reflexivity. Qed.\n",
    "\n",
    " Theorem replicate_length:\n",
    "\t forall n, length (replicate 1 n) = n.\n",
    " Proof. \n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity. \n",
    " - simpl. rewrite IHn. reflexivity.\n",
    " Qed. \n",
    " Theorem replicate_length_any: \n",
    "\t forall (X: Type) (x: X) n, length (replicate x n) = n. \n",
    " Proof.\n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity.\n",
    "- simpl. rewrite IHn. reflexivity.\n",
    " Qed.\n",
    "\\`\\`\\`\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b32ef9a7a2e4a8499522f020950d24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GenerationConfig, LlamaTokenizer, BitsAndBytesConfig\n",
    "\n",
    "base_model = \"/data/text-generation-webui/models/vicuna-7b\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.70 GiB total capacity; 22.35 GiB already allocated; 317.19 MiB free; 22.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m\n\u001b[1;32m     18\u001b[0m generate_params \u001b[39m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: input_ids,\n\u001b[1;32m     20\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration_config\u001b[39m\u001b[39m\"\u001b[39m: generation_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmax_new_tokens\u001b[39m\u001b[39m\"\u001b[39m: max_new_tokens,\n\u001b[1;32m     24\u001b[0m         }\n\u001b[1;32m     25\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 26\u001b[0m         generation_output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     27\u001b[0m                 input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     28\u001b[0m                 generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m     29\u001b[0m                 return_dict_in_generate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     30\u001b[0m                 output_scores\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     31\u001b[0m                 max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens,\n\u001b[1;32m     32\u001b[0m             )\n\u001b[1;32m     33\u001b[0m         s \u001b[39m=\u001b[39m generation_output\u001b[39m.\u001b[39msequences[\u001b[39m0\u001b[39m]\n\u001b[1;32m     34\u001b[0m         output \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(s)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/generation/utils.py:1524\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1523\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1524\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1525\u001b[0m         input_ids,\n\u001b[1;32m   1526\u001b[0m         beam_scorer,\n\u001b[1;32m   1527\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1528\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1529\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1530\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1531\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1532\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1533\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1534\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1535\u001b[0m     )\n\u001b[1;32m   1537\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1538\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/generation/utils.py:2810\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2806\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   2808\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2810\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2811\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2812\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2813\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2814\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2815\u001b[0m )\n\u001b[1;32m   2817\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2818\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:687\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    686\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    688\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    689\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    690\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    691\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    692\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    693\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    694\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    695\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    696\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    697\u001b[0m )\n\u001b[1;32m    699\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    700\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:577\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    569\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    570\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    571\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    578\u001b[0m         hidden_states,\n\u001b[1;32m    579\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    580\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    581\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    582\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    583\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    584\u001b[0m     )\n\u001b[1;32m    586\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    588\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:292\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    289\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    291\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    293\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    294\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    295\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    296\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    297\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    298\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    300\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    302\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:214\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    210\u001b[0m     value_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_states], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    212\u001b[0m past_key_value \u001b[39m=\u001b[39m (key_states, value_states) \u001b[39mif\u001b[39;00m use_cache \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_states, key_states\u001b[39m.\u001b[39;49mtranspose(\u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m)) \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m attn_weights\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, q_len, kv_seq_len):\n\u001b[1;32m    217\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    218\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttention weights should be of size \u001b[39m\u001b[39m{\u001b[39;00m(bsz\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\u001b[39m \u001b[39mq_len,\u001b[39m \u001b[39mkv_seq_len)\u001b[39m}\u001b[39;00m\u001b[39m, but is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mattn_weights\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.70 GiB total capacity; 22.35 GiB already allocated; 317.19 MiB free; 22.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "prompt = systemText + \"\\n\" + \"### Human: Write a Coq function to reverse a list.\\n###Assistant: \"\n",
    "device = \"cuda\"\n",
    "temperature = 0.7\n",
    "top_p = 0.75\n",
    "top_k = 40\n",
    "num_beams = 4\n",
    "max_new_tokens = 500\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams\n",
    "        )\n",
    "\n",
    "generate_params = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"generation_config\": generation_config,\n",
    "            \"return_dict_in_generate\": True,\n",
    "            \"output_scores\": True,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "        }\n",
    "with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                generation_config=generation_config,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "        s = generation_output.sequences[0]\n",
    "        output = tokenizer.decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Human: what color is the sky?\n",
      "###Assistant: \n",
      "\n",
      "The color of the sky can vary depending on the time of day and the weather conditions. During the daytime, when the sun is shining, the sky is usually a bright blue color. However, when the sun is setting or when there are clouds in the sky, the color of the sky can range from pink to orange to purple.</s><s>\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936c66a355784bae9500e061ce1183d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m quantization_config \u001b[39m=\u001b[39m BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m model_8bit \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      9\u001b[0m     model_id,\n\u001b[1;32m     10\u001b[0m     device_map\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     quantization_config\u001b[39m=\u001b[39mquantization_config,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_id)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:702\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    700\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_candidate\u001b[39m}\u001b[39;00m\u001b[39m does not exist or is not currently imported.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    701\u001b[0m         )\n\u001b[0;32m--> 702\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    704\u001b[0m \u001b[39m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[39m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1811\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1809\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1811\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1812\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1813\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1814\u001b[0m     init_configuration,\n\u001b[1;32m   1815\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1816\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1817\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1818\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1819\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1820\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1821\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1965\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1963\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1965\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1966\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1967\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1968\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1969\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1970\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama_fast.py:89\u001b[0m, in \u001b[0;36mLlamaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     80\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     81\u001b[0m     vocab_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     88\u001b[0m ):\n\u001b[0;32m---> 89\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     90\u001b[0m         vocab_file\u001b[39m=\u001b[39;49mvocab_file,\n\u001b[1;32m     91\u001b[0m         tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m     92\u001b[0m         clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m     93\u001b[0m         unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m     94\u001b[0m         bos_token\u001b[39m=\u001b[39;49mbos_token,\n\u001b[1;32m     95\u001b[0m         eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m     96\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_save_slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:114\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m TokenizerFast\u001b[39m.\u001b[39mfrom_file(fast_tokenizer_file)\n\u001b[1;32m    112\u001b[0m \u001b[39melif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[39m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[39m# We need to create and convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1288\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn instance of tokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_name\u001b[39m}\u001b[39;00m\u001b[39m cannot be converted in a Fast tokenizer instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m No converter was found. Currently available slow->fast convertors:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1283\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1284\u001b[0m     )\n\u001b[1;32m   1286\u001b[0m converter_class \u001b[39m=\u001b[39m SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[0;32m-> 1288\u001b[0m \u001b[39mreturn\u001b[39;00m converter_class(transformer_tokenizer)\u001b[39m.\u001b[39mconverted()\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:445\u001b[0m, in \u001b[0;36mSpmConverter.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    441\u001b[0m requires_backends(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprotobuf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    443\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs)\n\u001b[0;32m--> 445\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m sentencepiece_model_pb2 \u001b[39mas\u001b[39;00m model_pb2\n\u001b[1;32m    447\u001b[0m m \u001b[39m=\u001b[39m model_pb2\u001b[39m.\u001b[39mModelProto()\n\u001b[1;32m    448\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_tokenizer\u001b[39m.\u001b[39mvocab_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/utils/sentencepiece_model_pb2.py:91\u001b[0m\n\u001b[1;32m     25\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[1;32m     28\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mFileDescriptor(\n\u001b[1;32m     29\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentencepiece_model.proto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m     package\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentencepiece\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     ),\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     84\u001b[0m _TRAINERSPEC_MODELTYPE \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mEnumDescriptor(\n\u001b[1;32m     85\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModelType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     full_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentencepiece.TrainerSpec.ModelType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m     filename\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m     file\u001b[39m=\u001b[39mDESCRIPTOR,\n\u001b[1;32m     89\u001b[0m     create_key\u001b[39m=\u001b[39m_descriptor\u001b[39m.\u001b[39m_internal_create_key,\n\u001b[1;32m     90\u001b[0m     values\u001b[39m=\u001b[39m[\n\u001b[0;32m---> 91\u001b[0m         _descriptor\u001b[39m.\u001b[39;49mEnumValueDescriptor(\n\u001b[1;32m     92\u001b[0m             name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mUNIGRAM\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     93\u001b[0m             index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     94\u001b[0m             number\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     95\u001b[0m             serialized_options\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     96\u001b[0m             \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     97\u001b[0m             create_key\u001b[39m=\u001b[39;49m_descriptor\u001b[39m.\u001b[39;49m_internal_create_key,\n\u001b[1;32m     98\u001b[0m         ),\n\u001b[1;32m     99\u001b[0m         _descriptor\u001b[39m.\u001b[39mEnumValueDescriptor(\n\u001b[1;32m    100\u001b[0m             name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBPE\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    101\u001b[0m             index\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    102\u001b[0m             number\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    103\u001b[0m             serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    104\u001b[0m             \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    105\u001b[0m             create_key\u001b[39m=\u001b[39m_descriptor\u001b[39m.\u001b[39m_internal_create_key,\n\u001b[1;32m    106\u001b[0m         ),\n\u001b[1;32m    107\u001b[0m         _descriptor\u001b[39m.\u001b[39mEnumValueDescriptor(\n\u001b[1;32m    108\u001b[0m             name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWORD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m             index\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    110\u001b[0m             number\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m    111\u001b[0m             serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    112\u001b[0m             \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    113\u001b[0m             create_key\u001b[39m=\u001b[39m_descriptor\u001b[39m.\u001b[39m_internal_create_key,\n\u001b[1;32m    114\u001b[0m         ),\n\u001b[1;32m    115\u001b[0m         _descriptor\u001b[39m.\u001b[39mEnumValueDescriptor(\n\u001b[1;32m    116\u001b[0m             name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCHAR\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    117\u001b[0m             index\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m    118\u001b[0m             number\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[1;32m    119\u001b[0m             serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    120\u001b[0m             \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    121\u001b[0m             create_key\u001b[39m=\u001b[39m_descriptor\u001b[39m.\u001b[39m_internal_create_key,\n\u001b[1;32m    122\u001b[0m         ),\n\u001b[1;32m    123\u001b[0m     ],\n\u001b[1;32m    124\u001b[0m     containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m     serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    126\u001b[0m     serialized_start\u001b[39m=\u001b[39m\u001b[39m1294\u001b[39m,\n\u001b[1;32m    127\u001b[0m     serialized_end\u001b[39m=\u001b[39m\u001b[39m1347\u001b[39m,\n\u001b[1;32m    128\u001b[0m )\n\u001b[1;32m    129\u001b[0m _sym_db\u001b[39m.\u001b[39mRegisterEnumDescriptor(_TRAINERSPEC_MODELTYPE)\n\u001b[1;32m    131\u001b[0m _MODELPROTO_SENTENCEPIECE_TYPE \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mEnumDescriptor(\n\u001b[1;32m    132\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    133\u001b[0m     full_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentencepiece.ModelProto.SentencePiece.Type\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     serialized_end\u001b[39m=\u001b[39m\u001b[39m2184\u001b[39m,\n\u001b[1;32m    191\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/google/protobuf/descriptor.py:796\u001b[0m, in \u001b[0;36mEnumValueDescriptor.__new__\u001b[0;34m(cls, name, index, number, type, options, serialized_options, create_key)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, name, index, number,\n\u001b[1;32m    794\u001b[0m             \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,  \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m    795\u001b[0m             options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, create_key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 796\u001b[0m   _message\u001b[39m.\u001b[39;49mMessage\u001b[39m.\u001b[39;49m_CheckCalledFromGeneratedFile()\n\u001b[1;32m    797\u001b[0m   \u001b[39m# There is no way we can build a complete EnumValueDescriptor with the\u001b[39;00m\n\u001b[1;32m    798\u001b[0m   \u001b[39m# given parameters (the name of the Enum is not known, for example).\u001b[39;00m\n\u001b[1;32m    799\u001b[0m   \u001b[39m# Fortunately generated files just pass it to the EnumDescriptor()\u001b[39;00m\n\u001b[1;32m    800\u001b[0m   \u001b[39m# constructor, which will ignore it, so returning None is good enough.\u001b[39;00m\n\u001b[1;32m    801\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "# loading Llama\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model_id = \"/data/text-generation-webui/models/vicuna-7b\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Llama Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a Coq function to remove first and last occurrence of a given character from the string.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstOne = dataset[0]['specification']\n",
    "firstOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensor on device cpu is not on the expected device meta!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(input_text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Generate text\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m), max_length\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, no_repeat_ngram_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Decode the output\u001b[39;00m\n\u001b[1;32m      9\u001b[0m generated_text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m), skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/generation/utils.py:1485\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1478\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1479\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1480\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1481\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1482\u001b[0m     )\n\u001b[1;32m   1484\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1485\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1486\u001b[0m         input_ids,\n\u001b[1;32m   1487\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1488\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1489\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1490\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1491\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1492\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1493\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1494\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1495\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1496\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1497\u001b[0m     )\n\u001b[1;32m   1499\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1500\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/generation/utils.py:2524\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2523\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2524\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2525\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2526\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2527\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2528\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2531\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2532\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:687\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    686\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    688\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    689\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    690\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    691\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    692\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    693\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    694\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    695\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    696\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    697\u001b[0m )\n\u001b[1;32m    699\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    700\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:577\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    569\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    570\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    571\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    578\u001b[0m         hidden_states,\n\u001b[1;32m    579\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    580\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    581\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    582\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    583\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    584\u001b[0m     )\n\u001b[1;32m    586\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    588\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:289\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39m    past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 289\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layernorm(hidden_states)\n\u001b[1;32m    291\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[1;32m    292\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(\n\u001b[1;32m    293\u001b[0m     hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    294\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    299\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:91\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype \u001b[39min\u001b[39;00m [torch\u001b[39m.\u001b[39mfloat16, torch\u001b[39m.\u001b[39mbfloat16]:\n\u001b[1;32m     89\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m---> 91\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight \u001b[39m*\u001b[39;49m hidden_states\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/_prims_common/wrappers.py:220\u001b[0m, in \u001b[0;36mout_wrapper.<locals>._out_wrapper.<locals>._fn\u001b[0;34m(out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    218\u001b[0m             kwargs[k] \u001b[39m=\u001b[39m out_attr\n\u001b[0;32m--> 220\u001b[0m result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    221\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    222\u001b[0m     \u001b[39misinstance\u001b[39m(result, TensorLike)\n\u001b[1;32m    223\u001b[0m     \u001b[39mand\u001b[39;00m is_tensor\n\u001b[1;32m    224\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(result, Tuple)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(out_names)\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[39m# Naively you might expect this assert to be true, but\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39m# it's not:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[39m# be a normal meta tensor, but this is perfectly\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[39m# harmless.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/_prims_common/wrappers.py:130\u001b[0m, in \u001b[0;36melementwise_type_promotion_wrapper.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m promoted_args \u001b[39m=\u001b[39m {\n\u001b[1;32m    124\u001b[0m     x: _maybe_convert_to_dtype(bound\u001b[39m.\u001b[39marguments[x], compute_dtype)\n\u001b[1;32m    125\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_promoting_arg_names  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[39mif\u001b[39;00m x \u001b[39min\u001b[39;00m bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m    127\u001b[0m }\n\u001b[1;32m    128\u001b[0m bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mupdate(promoted_args)\n\u001b[0;32m--> 130\u001b[0m result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbound\u001b[39m.\u001b[39;49marguments)\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, TensorLike):\n\u001b[1;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m _maybe_convert_to_dtype(result, result_dtype)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/_refs/__init__.py:926\u001b[0m, in \u001b[0;36m_make_elementwise_binary_reference.<locals>.inner.<locals>._ref\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    919\u001b[0m check(\n\u001b[1;32m    920\u001b[0m     supports_two_python_scalars\n\u001b[1;32m    921\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(a, Number) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(b, Number)),\n\u001b[1;32m    922\u001b[0m     \u001b[39mlambda\u001b[39;00m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m: Receive two Number inputs to an elementwise binary operation!\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    923\u001b[0m     \u001b[39mValueError\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m )\n\u001b[1;32m    925\u001b[0m a, b \u001b[39m=\u001b[39m _maybe_broadcast(a, b)\n\u001b[0;32m--> 926\u001b[0m \u001b[39mreturn\u001b[39;00m prim(a, b)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/_refs/__init__.py:1532\u001b[0m, in \u001b[0;36mmul\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[39m@_make_elementwise_binary_reference\u001b[39m(\n\u001b[1;32m   1528\u001b[0m     type_promotion_kind\u001b[39m=\u001b[39mELEMENTWISE_TYPE_PROMOTION_KIND\u001b[39m.\u001b[39mDEFAULT,\n\u001b[1;32m   1529\u001b[0m     supports_two_python_scalars\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1530\u001b[0m )\n\u001b[1;32m   1531\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmul\u001b[39m(a: TensorLikeType, b: TensorLikeType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TensorLikeType:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m prims\u001b[39m.\u001b[39;49mmul(a, b)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/_ops.py:287\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/_prims/__init__.py:346\u001b[0m, in \u001b[0;36m_elementwise_meta\u001b[0;34m(type_promotion, args_with_fixed_dtypes, *args)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m args_with_fixed_dtypes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     args_ \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(args_with_fixed_dtypes) \u001b[39m+\u001b[39m args_\n\u001b[0;32m--> 346\u001b[0m utils\u001b[39m.\u001b[39;49mcheck_same_device(\u001b[39m*\u001b[39;49margs_, allow_cpu_scalar_tensors\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    347\u001b[0m utils\u001b[39m.\u001b[39mcheck_same_shape(\u001b[39m*\u001b[39margs_, allow_cpu_scalar_tensors\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    349\u001b[0m strides \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcompute_elementwise_output_strides(\u001b[39m*\u001b[39margs_)\n",
      "File \u001b[0;32m~/mambaforge/envs/python3.9/lib/python3.9/site-packages/torch/_prims_common/__init__.py:596\u001b[0m, in \u001b[0;36mcheck_same_device\u001b[0;34m(allow_cpu_scalar_tensors, *args)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39m!=\u001b[39m arg\u001b[39m.\u001b[39mdevice:\n\u001b[1;32m    589\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    590\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTensor on device \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    591\u001b[0m             \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(arg\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    595\u001b[0m         )\n\u001b[0;32m--> 596\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    597\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    599\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnexpected type when checking for same device, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(arg)) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor on device cpu is not on the expected device meta!"
     ]
    }
   ],
   "source": [
    "# Encode the input text\n",
    "input_text = firstOne\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, do_sample=True)\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(output[0].to(\"cpu\"), skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemText = \"\"\" You are an AI assistant helping users write Coq code in order to implement given function specifications. \n",
    "1. The program you write should only contain Coq code in response to the given function specification. \n",
    "3. Any step-by-step reasoning that is not Coq code should be written as a comment.\n",
    "3. As the user provides compiler feedback, modify and update the program accordingly and keep the variables and the general program structure consistent.\n",
    "4. In addition to implementing the function, give at least 2 properties as theorems with their proofs.\n",
    "\n",
    "The following are examples.\n",
    "\n",
    "Query from user:\n",
    "Write a function that creates a list of n elements. Test case 1: replicate 1 0 = []. Test case 2: replicate 1 2 = [1; 1]. \n",
    "\n",
    "Response from assistant:\n",
    "\\`\\`\\`\n",
    "Require Import Coq.Lists.List. \n",
    " Open Scope list_scope. \n",
    " Import ListNotations. \n",
    " Fixpoint replicate (x: X) (n: nat): list X := \n",
    " match n with \n",
    " | 0 => []\n",
    " | S n => x :: replicate x n \n",
    " end. \n",
    " Lemma example_replicate_0: replicate 1 0 = []. \n",
    " Proof. simpl. reflexivity. Qed.\n",
    " Lemma example_replicate_2: replicate 1 2 = [1; 1].\n",
    " Proof. simpl. reflexivity. Qed.\n",
    "\n",
    " Theorem replicate_length:\n",
    "\t forall n, length (replicate 1 n) = n.\n",
    " Proof. \n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity. \n",
    " - simpl. rewrite IHn. reflexivity.\n",
    " Qed. \n",
    " Theorem replicate_length_any: \n",
    "\t forall (X: Type) (x: X) n, length (replicate x n) = n. \n",
    " Proof.\n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity.\n",
    "- simpl. rewrite IHn. reflexivity.\n",
    " Qed.\n",
    "\\`\\`\\`\n",
    "\n",
    "Query from user:\n",
    "Your code produces an error in the line Fixpoint replicate (x: X) (n: nat): list X :=\\n{}Can you please explain what this error means? Let's think step by step. Please rewrite all code if you rewrite any code.\n",
    "File \\\"./ex.v\\\", line 4, characters 24-25:\\nError: The reference X was not found in the current environment.\n",
    "\n",
    "Response from assistant:\n",
    "\\`\\`\\`\n",
    "Require Import Coq.Lists.List. \n",
    " Open Scope list_scope. \n",
    " Import ListNotations. \n",
    " Fixpoint replicate '{X: Type'} (x: X) (n: nat): list X := \n",
    " match n with \n",
    " | 0 => []\n",
    " | S n => x :: replicate x n \n",
    " end. \n",
    " Lemma example_replicate_0: replicate 1 0 = []. \n",
    " Proof. simpl. reflexivity. Qed.\n",
    " Lemma example_replicate_2: replicate 1 2 = [1; 1].\n",
    " Proof. simpl. reflexivity. Qed.\n",
    "\n",
    " Theorem replicate_length:\n",
    "\t forall n, length (replicate 1 n) = n.\n",
    " Proof. \n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity. \n",
    " - simpl. rewrite IHn. reflexivity.\n",
    " Qed. \n",
    " Theorem replicate_length_any: \n",
    "\t forall (X: Type) (x: X) n, length (replicate x n) = n. \n",
    " Proof.\n",
    " intros. \n",
    " induction n.\n",
    " - simpl. reflexivity.\n",
    "- simpl. rewrite IHn. reflexivity.\n",
    " Qed.\n",
    "\\`\\`\\`\"\"\"\n",
    "\n",
    "messages=[{\"role\": \"system\", \"content\": systemText}]\n",
    "\n",
    "def generate(q):\n",
    "  '''\n",
    "  Generate output from the correct model and clean it from pre- and post- rambles if possible.\n",
    "  ''' \n",
    "  # make this script retry if the connection is rejected for some reason\n",
    "  while True:\n",
    "    try:\n",
    "      messages.append({\"role\": \"user\", \"content\": q})\n",
    "      response = openai.ChatCompletion.create(\n",
    "                    model='gpt-4-0314', \n",
    "                    messages=messages)\n",
    "      response = response.choices[0].message.content\n",
    "      messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "      \n",
    "      # clean the response if possible\n",
    "      c_response = response\n",
    "      try:\n",
    "        match = re.search('```coq(.*?)```', c_response, re.DOTALL)\n",
    "        c_response = match.group(1)\n",
    "      except:\n",
    "        pass\n",
    "      try:\n",
    "        match = re.search('```(.*?)```', c_response, re.DOTALL)\n",
    "        c_response = match.group(1)\n",
    "      except:\n",
    "        pass\n",
    "      return c_response\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "def run_trial(q_core, pid, outfile, verbose=True, ntrials=10):\n",
    "  '''\n",
    "  Runs one trial on one prompt. \n",
    "  - q: function spec with test cases\n",
    "  - pid: the prompt id\n",
    "  '''\n",
    "  q = q_core\n",
    "  if verbose:\n",
    "    print(\"The task: {}\".format(q))\n",
    "\n",
    "  for t in range(ntrials): \n",
    "    # for recording the dataset\n",
    "    out = {\n",
    "            \"prompt_id\": pid,\n",
    "            \"iteration\": t,\n",
    "            \"instruction\": q,\n",
    "            \"output\": None,\n",
    "            \"compiler_feedback\": None,\n",
    "            \"stats\": {\n",
    "                        \"total_lines\" : None,\n",
    "                        \"compiled_lines\": None,\n",
    "                        \"percent_compiled\": None\n",
    "                    }\n",
    "            }\n",
    "\n",
    "    # generate model response\n",
    "    response = generate(q)\n",
    "\n",
    "    # get compiler feedback\n",
    "    cf = cfeedback(response)\n",
    "\n",
    "    if verbose:\n",
    "      print(\"-----Attempt {}---------\".format(t))\n",
    "      print(response)\n",
    "\n",
    "    if cf is not None:\n",
    "      line_number = get_linenumber(cf) - 1\n",
    "      total_lines = get_totallines(response)\n",
    "      percent_compiled = (line_number)/total_lines\n",
    "      linetxt = get_line(line_number, response)\n",
    "\n",
    "      # get the model to reflect on the error\n",
    "      q = \"Your code produces an error in the line {}\\n{}Can you please explain what this error means? Let's think step by step. Please rewrite all code if you rewrite any code.\"\\\n",
    "        .format(linetxt, cf)\n",
    "      if verbose:\n",
    "        print(q)\n",
    "        print(percent_compiled)\n",
    "    else:\n",
    "      total_lines = get_totallines(response)\n",
    "      line_number = total_lines\n",
    "      percent_compiled = 1.0\n",
    "      q = \"The model solved the problem!\"\n",
    "      if verbose:\n",
    "        print(q)\n",
    "        print(percent_compiled)\n",
    "\n",
    "    # append all data to json lines file\n",
    "    out[\"output\"] = response\n",
    "    out[\"compiler_feedback\"] = cf\n",
    "    out[\"stats\"][\"total_lines\"] = total_lines\n",
    "    out[\"stats\"][\"compiled_lines\"] = line_number\n",
    "    out[\"stats\"][\"percent_compiled\"] = percent_compiled\n",
    "\n",
    "    with open(outfile, 'a') as file:\n",
    "      file.write(json.dumps(out) + '\\n')\n",
    "    if verbose:\n",
    "      print(\"recorded in {}\".format(outfile))\n",
    "\n",
    "    # don't continue if model has completely solved problem\n",
    "    if cf is None:\n",
    "      break\n",
    "\n",
    "  return None\n",
    "\n",
    "def main():\n",
    "  pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n",
    "  outfile = \"Dolly_EasyMediumHard01.ndjson\"\n",
    "  # run_trial(q, 0, outfile)\n",
    "  for i in range(0,3):\n",
    "    q = dataset[i]['query'] \n",
    "    run_trial(q, i, outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
